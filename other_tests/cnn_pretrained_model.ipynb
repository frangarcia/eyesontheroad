{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de fatiga en la conducción de vehículos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción\n",
    "Auctorvolumus nostrum iaculis mus torquent ac atqui vehicula scelerisque at nonumy.  Luctusaliquet ponderum nisl mea scelerisque nec graeci.\n",
    "Torquentac in dicant scripserit oratio regione comprehensam nonumy auctor aliquid conclusionemque delicata periculis fames quem sale iusto euripidis.  Erremvolutpat tempus sed lorem habitasse legere conceptam oporteat similique facilisis.  Nonposse usu erat ea salutatus suspendisse.  Aliquetbrute doctus fastidii moderatius ignota vero mus.  Utamurpurus lacinia ex antiopam ne deserunt.  Comprehensammetus voluptatum praesent egestas consul.\n",
    "\n",
    "\n",
    "## Objetivo\n",
    "Auctorvolumus nostrum iaculis mus torquent ac atqui vehicula scelerisque at nonumy.  Luctusaliquet ponderum nisl mea scelerisque nec graeci.\n",
    "Torquentac in dicant scripserit oratio regione comprehensam nonumy auctor aliquid conclusionemque delicata periculis fames quem sale iusto euripidis.  Erremvolutpat tempus sed lorem habitasse legere conceptam oporteat similique facilisis.  Nonposse usu erat ea salutatus suspendisse.  Aliquetbrute doctus fastidii moderatius ignota vero mus.  Utamurpurus lacinia ex antiopam ne deserunt.  Comprehensammetus voluptatum praesent egestas consul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "UNSPLITTED_FOLDER_IMAGES = './datasets/dmd/labels'\n",
    "SPLITTED_FOLDER_IMAGES = './datasets/dmd/labels_split'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos las imágenes en train, validación y test en un ratio de:\n",
    "* 70% train.\n",
    "* 20% validación.\n",
    "* 10% test del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install split_folders\n",
    "import splitfolders\n",
    "import os\n",
    "if not os.path.exists(SPLITTED_FOLDER_IMAGES):\n",
    "    splitfolders.ratio(UNSPLITTED_FOLDER_IMAGES, SPLITTED_FOLDER_IMAGES, seed=1, ratio=(.7, .2, .1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis exploratorio de datos\n",
    "Antes de construir el modelo, es fundamental llevar a cabo un proceso de exploración de los datos para identificar y resolver cualquier posible problema. Durante este análisis, verificaremos el equilibrio entre las diferentes clases para evitar cualquier sesgo que pueda afectar a nuestro modelo. Además, nos aseguraremos de que todas las imágenes tengan tres canales y, en caso necesario, normalizaremos los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset de train, test y validación\n",
    "Para crear los dataset de train y test, vamos a utilizar la función splitfolders. Como ya hemos descargado algunas fotos de internet para la validación, únicamente dividiremos nuestro dataset original en train y test con una proporción de 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (224, 224, 3)\n",
    "TRAIN_DIR = SPLITTED_FOLDER_IMAGES + '/train'\n",
    "VAL_DIR = SPLITTED_FOLDER_IMAGES + '/val'\n",
    "TEST_DIR = SPLITTED_FOLDER_IMAGES + '/test'\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (180, 180)\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "#!pip install chardet\n",
    "#!pip install --upgrade charset_normalizer\n",
    "import tensorflow as tf\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    validation_split=VAL_SPLIT,\n",
    "    subset='training',\n",
    "    color_mode='rgb',\n",
    "    seed=1,\n",
    "    shuffle=False,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    VAL_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    validation_split=VAL_SPLIT,\n",
    "    subset='validation',\n",
    "    color_mode='rgb',\n",
    "    seed=1,\n",
    "    shuffle=False,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clases equilibradas\n",
    "Para empezar, verificamos la distribución equilibrada de las clases, lo cual implica que todas ellas contengan la misma cantidad de imágenes. Para llevar a cabo esta comprobación, hemos implementado la función \"check_classes\", que realiza el análisis y nos proporciona un mensaje indicando si las clases están balanceadas o no, junto con el tamaño de cada una de ellas. Como entrada, solo necesitamos especificar la ruta principal y las diferentes carpetas que contienen las imágenes correspondientes a cada clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def showImageGrid(train_ds):\n",
    "\n",
    "    # Get a batch of images from the dataset\n",
    "    image_batch, label_batch = next(iter(train_ds))\n",
    "\n",
    "    # Convert image_batch and label_batch to numpy arrays\n",
    "    image_batch = image_batch.numpy()\n",
    "    label_batch = label_batch.numpy()\n",
    "\n",
    "    # Randomly select 10 images from the batch\n",
    "    random_indices = np.random.choice(range(len(image_batch)), size=5, replace=False)\n",
    "    random_images = image_batch[random_indices]\n",
    "    random_labels = label_batch[random_indices]\n",
    "\n",
    "    # Create a grid for displaying the images\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(10, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Iterate over the random images and labels\n",
    "    for i, (image, label) in enumerate(zip(random_images, random_labels)):\n",
    "        # Display the image\n",
    "        axes[i].imshow(image.astype(np.uint8))\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(np.argmax(label))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "showImageGrid(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según podemos observar en la salida previa, las clases presentan una distribución equilibrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Número de canales\n",
    "Además, es crucial verificar que todas las imágenes tengan tres canales para evitar posibles inconvenientes más adelante. Para facilitar esta comprobación, hemos implementado la función \"check_channels\". Esta función requiere como entrada la ruta principal, los nombres de las subcarpetas y el número de canales que deseamos que tengan las imágenes, en este caso, 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifyChannels(train_ds):\n",
    "    for image_batch, label_batch in train_ds:\n",
    "        for image in image_batch:\n",
    "            num_channels = image.shape[-1]  # Get the number of channels\n",
    "            if num_channels != 3:\n",
    "                print(\"Image does not have 3 channels:\", image.shape)\n",
    "        break\n",
    "    print(\"All images have 3 channels\")\n",
    "\n",
    "verifyChannels(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización\n",
    "Por último, en cuanto a la normalización, no sería necesario llevarla a cabo ya que las redes que vamos a utilizar ya contienen una función de preprocesado que importamos directamente desde keras y que realiza todo el tratamiento de los datos para adaptar el input a la red neuronal en cuestión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización VGG19\n",
    "Comenzamos la modelización con la red VGG19 y añadimos las capas necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "#Create VGG19 preprocessing\n",
    "train_ds = train_ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Preprocess the images using ImageDataGenerator\n",
    "# train_ds = train_ds.map(lambda x, y: (datagen.preprocess_input(x), y))\n",
    "# val_ds = val_ds.map(lambda x, y: (imageDataGen.preprocess_input(x), y))\n",
    "\n",
    "showImageGrid(train_ds)\n",
    "showImageGrid(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T10:39:04.751469Z",
     "start_time": "2023-05-27T10:39:04.282781Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications import VGG19\n",
    "\n",
    "base_model=VGG19(weights='imagenet', include_top=False)\n",
    "\n",
    "vgg = base_model.output\n",
    "vgg = GlobalAveragePooling2D()(vgg)\n",
    "vgg = Dense(512, activation='relu')(vgg)\n",
    "outputs = Dense(num_classes, activation='softmax')(vgg)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=IMG_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el modelo y obtenemos el summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T10:39:07.888582Z",
     "start_time": "2023-05-27T10:39:07.853772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 512)               0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 3591      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20290631 (77.40 MB)\n",
      "Trainable params: 20290631 (77.40 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# conda install -c conda-forge python-graphviz\n",
    "#!pip install keras_visualizer\n",
    "#!pip install pip install git+https://github.com/raghakot/keras-vis.git -U\n",
    "from keras.models import Model\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "# from keras_visualizer import visualizer\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=outputs)\n",
    "model.summary()\n",
    "# visualizer(model,file_name='modelVisualization.png', file_format='png', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Modelo base:\", len(base_model.layers), \"\\nModelo:\", len(model.layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicamos a partir de qué capa el modelo empieza a entrenar. Esto es importante porque si lo entrenamos de 0\n",
    "nos va a llevar mucho más tiempo. Así , el model sólo aprenderá de las capas que le indiquemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:22]:\n",
    "    layer.trainable=False\n",
    "for layer in model.layers[22:]:\n",
    "    layer.trainable=True\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos como el número de parámetros entrenables es de 20.288.579, correspondientes a las capas que no hemos congelado y que vamos a entrenar a continuación. Para ello, creamos el generador de train y test usando como función de preproceso la de la red VGG19, fijamos los directorios donde se encuentran los dataset e indicamos como tamaño objetivo 224x224:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilamos el modelo utilizando como optimizador \"Adam\" y como función de coste la entropía cruzada (Cross-entropy). Lo entrenaremos para 50 épocas pero activamos los callbacks de 'early_stopping' y 'best_model_checkpoint':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T09:33:19.814749Z",
     "start_time": "2023-05-27T09:33:19.800968Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T10:30:53.085245Z",
     "start_time": "2023-05-27T10:30:52.548103Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "MODEL_SAVE_FOLDER = './models/cnn/cnn_vgg/'\n",
    "\n",
    "# if os.path.exists(MODEL_SAVE_FOLDER):\n",
    "modelvgg19 =  load_model(MODEL_SAVE_FOLDER +'best_cnn_vgg_model_saturday')\n",
    "modelvgg19.hi\n",
    "# else:\n",
    "#     early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "#     best_model_checkpoint = tf.keras.callbacks.ModelCheckpoint(MODEL_SAVE_FOLDER +'best_cnn_vgg_model_saturday', monitor='loss', save_best_only=True, mode='min')\n",
    "#     modelvgg19 = model.fit(\n",
    "#         x=train_ds,\n",
    "#         validation_data = val_ds,\n",
    "#         epochs=10,\n",
    "#         callbacks=[early_stop,best_model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que el modelo ha sido entrenado, procedemos a visualizar gráficamente la precisión y la función de coste tanto para el conjunto de entrenamiento como para el conjunto de prueba. Esta representación gráfica nos permitirá analizar el rendimiento del modelo en ambas situaciones y evaluar su capacidad de generalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T10:28:22.054185Z",
     "start_time": "2023-05-27T10:28:22.023870Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m acc      \u001B[38;5;241m=\u001B[39m \u001B[43mmodelvgg19\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhistory\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43maccuracy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m      4\u001B[0m val_acc  \u001B[38;5;241m=\u001B[39m modelvgg19\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_accuracy\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      5\u001B[0m loss     \u001B[38;5;241m=\u001B[39m modelvgg19\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc      = modelvgg19.history['accuracy']\n",
    "val_acc  = modelvgg19.history['val_accuracy']\n",
    "loss     = modelvgg19.history['loss']\n",
    "val_loss = modelvgg19.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Entrenamiento acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validación acc')\n",
    "plt.title('Accuracy - exactitud de entrenamiento y validación')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Entrenamiento loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validación loss')\n",
    "plt.title('Loss - función objetivo en entrenamiento y prueba')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponderumquot utinam periculis facilis errem quam tortor sanctus prompta mucius wisi posse feugait quo ne invenire.  Mineglegentur his deterruisset docendi sadipscing tristique arcu in.  Natumex ocurreret saepe nibh viderer.  Ultricesvocibus donec minim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones\n",
    "Después de haber seleccionado el modelo, nos adentramos en la etapa de hacer predicciones utilizando las imágenes que hemos obtenido de Internet y que están almacenadas en la carpeta \"testeo\". Para llevar a cabo estas predicciones, hemos desarrollado una función que requiere como entrada la ruta al directorio de imágenes, la función de preprocesamiento elegida y el modelo seleccionado. Esta función nos devolverá las imágenes junto con sus respectivas predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
